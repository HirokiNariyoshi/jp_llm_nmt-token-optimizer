"""
Test with realistic longer prompt to see token optimization benefits
"""

from token_optimizer import TokenOptimizer
from deep_translator import GoogleTranslator
import tiktoken

# Realistic longer prompt
LONG_PROMPT = """
æ—¥æœ¬ã®ä¼æ¥­ãŒæ–°ã—ã„ã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹éš›ã«ã€
ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸¡ç«‹ã•ã›ã‚‹ãŸã‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’
è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«ã¤ã„ã¦ï¼š

1. ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼ã¨ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ã®å®Ÿè£…æ–¹æ³•
2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ
3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®é€šä¿¡ã®æš—å·åŒ–
4. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
5. ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã¨CSRFå¯¾ç­–

å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã¨ã¨ã‚‚ã«ã€ãã‚Œãã‚Œã®å®Ÿè£…ã«ãŠã‘ã‚‹æ³¨æ„ç‚¹ã‚‚å«ã‚ã¦
æ•™ãˆã¦ãã ã•ã„ã€‚
"""

def test_realistic():
    translator = GoogleTranslator(source='ja', target='en')
    enc = tiktoken.get_encoding("cl100k_base")
    
    # Translate and count tokens
    en_prompt = translator.translate(LONG_PROMPT)
    
    ja_tokens = len(enc.encode(LONG_PROMPT))
    en_tokens = len(enc.encode(en_prompt))
    
    savings = ja_tokens - en_tokens
    percent = (savings / ja_tokens) * 100
    
    print("=" * 70)
    print("ğŸ“ REALISTIC LONG PROMPT TEST")
    print("=" * 70)
    
    print(f"\nğŸ‡¯ğŸ‡µ Japanese Prompt:")
    print(LONG_PROMPT)
    print(f"\nğŸ“Š Japanese Tokens: {ja_tokens}")
    
    print(f"\nğŸ‡¬ğŸ‡§ English Translation:")
    print(en_prompt)
    print(f"\nğŸ“Š English Tokens: {en_tokens}")
    
    print(f"\nğŸ’¾ Token Savings: {savings} tokens ({percent:.1f}%)")
    
    if savings > 0:
        print("âœ… Optimization would save tokens on INPUT")
    else:
        print("âŒ Optimization would INCREASE tokens on INPUT")
    
    # Now test actual response
    print("\n" + "=" * 70)
    print("ğŸ”¬ Testing Actual Responses...")
    print("=" * 70)
    
    optimizer = TokenOptimizer(llm_model="qwen2.5:1.5b")
    
    print("\nâ³ Running optimized request...")
    result = optimizer.optimize_request(
        prompt=LONG_PROMPT,
        max_tokens=500,
        force_optimization=True
    )
    
    print(f"\nğŸ“Š Results:")
    print(f"   Original tokens: {result.metrics.original_tokens}")
    print(f"   Optimized tokens: {result.metrics.optimized_tokens}")
    print(f"   Tokens saved: {result.metrics.tokens_saved}")
    print(f"   Reduction: {result.metrics.token_reduction_percent:.1f}%")
    
    print(f"\nâ±ï¸  Timing:")
    print(f"   Translation: {result.metrics.translation_time:.2f}s")
    print(f"   LLM: {result.metrics.llm_time:.2f}s")
    print(f"   Total: {result.metrics.total_time:.2f}s")
    
    print(f"\nğŸ“ Response Preview:")
    print(result.content[:500] + "..." if len(result.content) > 500 else result.content)

if __name__ == "__main__":
    test_realistic()
