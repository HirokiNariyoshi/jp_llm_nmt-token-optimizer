"""
Japanese Query Optimizer - Main Interactive Script

Demonstrates token savings by translating Japanese queries to English
for processing by English-optimized LLMs.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from token_optimizer import TokenOptimizer

# Configuration
TEST_MODE = False  # Set to True to use hardcoded example
COMPARE_MODE = False  # Set to True to query both paths for accurate comparison (2x slower)

# Test prompt (long conversational example)
TEST_PROMPT = """
Êù•Êúà„ÄÅË¶™Âèã„ÅÆ„Çµ„Éó„É©„Ç§„Ç∫„Éê„Éº„Çπ„Éá„Éº„Éë„Éº„ÉÜ„Ç£„Éº„Çí‰ºÅÁîª„Åó„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ
ÂâµÈÄ†ÁöÑ„Å™„Ç¢„Ç§„Éá„Ç¢„ÅåÂøÖË¶Å„Åß„Åô„ÄÇÂΩºÂ•≥„ÅØ30Ê≠≥„Å´„Å™„Çä„ÄÅ„Ç¢„Ç¶„Éà„Éâ„Ç¢Ê¥ªÂãï„ÄÅ
„É¥„Ç£„É≥„ÉÜ„Éº„Ç∏„ÅÆÁæéÂ≠¶„ÄÅÊ§çÁâ©„ÇÑ„Ç¨„Éº„Éá„Éã„É≥„Ç∞„Å´Èñ¢ÈÄ£„Åô„Çã„Åì„Å®„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô„ÄÇ
‰ªï‰∫ã„ÅÆ„Çπ„Éà„É¨„Çπ„ÇÑÂÄã‰∫∫ÁöÑ„Å™Ë™≤È°å„ÅßÂ§ßÂ§â„Å™‰∏ÄÂπ¥„ÇíÈÅé„Åî„Åó„Å¶„Åç„Åü„ÅÆ„Åß„ÄÅ
Êú¨ÂΩì„Å´ÁâπÂà•„Å™„ÇÇ„ÅÆ„Å´„Åó„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ

„Éë„Éº„ÉÜ„Ç£„Éº„ÅØÁßÅ„ÅÆÂ∫≠„ÅßË°å„ÅÜ‰∫àÂÆö„Åß„Åô„ÄÇÂ∫≠„Å´„ÅØ„Åü„Åè„Åï„Çì„ÅÆËä±„ÇÑÊú®„Åå„ÅÇ„Çä„ÄÅ
„Åã„Å™„ÇäÂ∫É„ÅÑ„Çπ„Éö„Éº„Çπ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÂ§ïÊöÆ„ÇåÊôÇ„ÄÅ„Åä„Åù„Çâ„ÅèÂçàÂæå6ÊôÇÈ†É„Å´
‰Ωï„Åã„Åß„Åç„Çå„Å∞„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÁÖßÊòé„ÅåÊú¨ÂΩì„Å´Áæé„Åó„ÅÑÊôÇÈñìÂ∏Ø„Åß„Åô„ÄÇ
Á¥Ñ25‰∫∫„ÅåÊù•„Çã‰∫àÂÆö„Åß„ÄÅ„Åª„Å®„Çì„Å©„ÅåÂ§ßÂ≠¶ÊôÇ‰ª£„ÅÆË¶™„Åó„ÅÑÂèã‰∫∫„Å®„ÄÅ
ÂΩºÂ•≥„ÅåÊú¨ÂΩì„Å´Ê∞ó„Å´ÂÖ•„Å£„Å¶„ÅÑ„ÇãÂêåÂÉöÊï∞‰∫∫„Åß„Åô„ÄÇ

‰∫àÁÆó„ÅØ800„Éâ„É´„Åã„Çâ1000„Éâ„É´„Åß„ÄÅË±™ËèØ„ÅßÈ´ò‰æ°„Å™„ÇÇ„ÅÆ„Çà„Çä„ÇÇ„ÄÅ
Ë¶™ÂØÜ„ÅßÂÄã‰∫∫ÁöÑ„Å™Èõ∞Âõ≤Ê∞ó„Å´„Åó„Åü„ÅÑ„Å®ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇÂΩºÂ•≥„ÅØÂ§ßÈü≥Èáè„ÅÆÈü≥Ê•Ω„ÇÑ
Ê¥æÊâã„Å™„ÇÇ„ÅÆ„ÅØÂ•Ω„Åç„Åß„ÅØ„Å™„ÅÑ„ÅÆ„Åß„ÄÅËâØ„ÅÑÈ£ü‰∫ã„ÄÅÊ∏©„Åã„ÅÑ‰ºöË©±„ÄÅ„Åù„Åó„Å¶
ÁßÅ„Åü„Å°„Åø„Çì„Å™„Åå„Å©„Çå„Å†„ÅëÂΩºÂ•≥„ÇíÂ§ßÂàá„Å´ÊÄù„Å£„Å¶„ÅÑ„Çã„Åã„ÇíÁ§∫„Åô
ÂøÉ„ÅÆ„Åì„ÇÇ„Å£„ÅüÊºîÂá∫„ÅÆ„ÅÇ„Çã„ÄÅÂ±ÖÂøÉÂú∞„ÅÆËâØ„ÅÑ„Ç¨„Éº„Éá„É≥„Éë„Éº„ÉÜ„Ç£„Éº„Çí
ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ

Ë£ÖÈ£æ„ÄÅ„Ç¢„ÇØ„ÉÜ„Ç£„Éì„ÉÜ„Ç£„ÄÅÈ£ü‰∫ã„ÅÆÈÅ∏ÊäûËÇ¢„ÄÅ„Åù„Åó„Å¶ÂΩºÂ•≥„ÇíÊú¨ÂΩì„Å´ÊÑõ„Åï„Çå„Å¶„ÅÑ„Çã„Å®
ÊÑü„Åò„Åï„Åõ„ÇãÁâπÂà•„Å™Áû¨Èñì„ÇÑ„Çµ„Éó„É©„Ç§„Ç∫Ë¶ÅÁ¥†„ÅÆ„Ç¢„Ç§„Éá„Ç¢„Çí„Éñ„É¨„Ç§„É≥„Çπ„Éà„Éº„Éü„É≥„Ç∞
„Åô„Çã„ÅÆ„ÇíÊâã‰ºù„Å£„Å¶„ÅÑ„Åü„Å†„Åë„Åæ„Åô„ÅãÔºü„Åæ„Åü„ÄÅÂΩºÂ•≥„ÅØÈùûÂ∏∏„Å´Ë¶≥ÂØüÂäõ„Åå„ÅÇ„Çä„ÄÅ
ÈÄöÂ∏∏„ÅØÁâ©‰∫ã„ÇíË¶ãÊäú„ÅÑ„Å¶„Åó„Åæ„ÅÜ„ÅÆ„Åß„ÄÅÂΩºÂ•≥„Å´Ê∞ó„Å•„Åã„Çå„Åö„Å´„Åô„Åπ„Å¶„ÇíË™øÊï¥„Åô„Çã
„Åü„ÇÅ„ÅÆ„Éí„É≥„Éà„ÇÇ„ÅÑ„Åü„Å†„Åë„Çã„Å®Âä©„Åã„Çä„Åæ„Åô„ÄÇ
"""


def main():
    print("=" * 70)
    print("Japanese Query Optimizer")
    print("Reduces token usage by ~58% for English-optimized LLMs")
    print("=" * 70)
    print()
    
    # Initialize optimizer
    optimizer = TokenOptimizer(
        llm_provider="ollama",
        llm_model="qwen2.5:1.5b",
        translation_provider="google",
        cache_enabled=False,
        optimization_threshold=10
    )
    
    # Get prompt based on mode
    if TEST_MODE:
        print("üìù Using test mode with hardcoded prompt")
        print()
        japanese_prompt = TEST_PROMPT
        print("Prompt preview:")
        print(japanese_prompt[:200] + "...\n")
    else:
        print("üìù Enter your Japanese query (press Enter twice when done):")
        print()
        lines = []
        while True:
            line = input()
            if line == "" and len(lines) > 0:
                break
            if line:
                lines.append(line)
        japanese_prompt = "\n".join(lines)
        
        if not japanese_prompt.strip():
            print("‚ùå No prompt entered. Exiting.")
            return
        print()
    
    # Word count
    word_count = len(japanese_prompt.split())
    print(f"üìä Analyzing query ({word_count} words)...")
    print()
    
    # Analyze potential savings
    analysis = optimizer.analyze_potential_savings(japanese_prompt, output_tokens=500)
    
    print("Token Efficiency Analysis:")
    print(f"  Japanese tokens:  {analysis['japanese_input_tokens']}")
    print(f"  English tokens:   {analysis['english_input_tokens']}")
    print(f"  Tokens saved:     {analysis['input_tokens_saved']} ({analysis['token_reduction_percent']:.1f}%)")
    print(f"  Cost saved:       ${analysis['cost_saved']:.6f}")
    print(f"  Recommendation:   {analysis['recommendation']}")
    print()
    
    if analysis['input_tokens_saved'] <= 0:
        print("‚ö†Ô∏è  English translation would not save tokens for this query.")
        proceed = input("Continue anyway? (y/n): ")
        if proceed.lower() != 'y':
            return
        print()
    
    # Process request
    if COMPARE_MODE:
        print("üöÄ Processing with compare mode (querying both paths)...")
        print("   This queries the model twice for accurate measurement")
    else:
        print("üöÄ Processing with optimization...")
    print()
    
    response = optimizer.optimize_request(
        prompt=japanese_prompt,
        max_tokens=800 if TEST_MODE else 500,
        compare_mode=COMPARE_MODE
    )
    
    # Display results
    print("=" * 70)
    print("RESPONSE")
    print("=" * 70)
    print()
    
    # Show first 500 chars
    if len(response.content) > 500:
        print(response.content[:500] + "...")
        print(f"\n(Response truncated - {len(response.content)} total characters)")
    else:
        print(response.content)
    
    print()
    print("=" * 70)
    print("OPTIMIZATION METRICS")
    print("=" * 70)
    print()
    
    metrics = response.metrics
    
    # Token analysis
    print("üìä TOKEN USAGE:")
    print(f"  Original tokens:  {metrics.original_tokens}")
    print(f"  Optimized tokens: {metrics.optimized_tokens}")
    print(f"  Tokens saved:     {metrics.tokens_saved}")
    print(f"  Reduction:        {metrics.token_reduction_percent:.1f}%")
    print()
    
    # Cost analysis
    print("üí∞ COST SAVINGS:")
    print(f"  Without optimization: ${metrics.original_cost:.6f}")
    print(f"  With optimization:    ${metrics.optimized_cost:.6f}")
    print(f"  Saved:                ${metrics.cost_saved:.6f}")
    print(f"  Cost reduction:       {metrics.cost_reduction_percent:.1f}%")
    print()
    
    # Time analysis
    print("‚è±Ô∏è  PERFORMANCE:")
    print(f"  Translation time: {metrics.translation_time:.2f}s")
    print(f"  LLM time:         {metrics.llm_time:.2f}s")
    print(f"  Total time:       {metrics.total_time:.2f}s")
    print()
    
    # Status summary
    if COMPARE_MODE:
        print("‚ÑπÔ∏è  Compare mode: Token counts are from actual model responses")
        print("   (not tiktoken estimates)")
        print()
    
    if metrics.used_optimization:
        if metrics.tokens_saved > 0:
            print("‚úÖ Optimization successful! English translation reduced token usage.")
            print(f"   For high-volume applications, this saves ~{metrics.token_reduction_percent:.1f}% on API costs.")
        else:
            print("‚ö†Ô∏è  Optimization used but no savings (English similar to Japanese for this query)")
    else:
        print("‚ÑπÔ∏è  Direct Japanese used (was more efficient than translating)")
    
    print()
    print("=" * 70)


if __name__ == "__main__":
    print("Make sure Ollama is running: ollama serve")
    print()
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ùå Cancelled by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
